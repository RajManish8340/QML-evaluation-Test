{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Task IX: Kolmogorov-Arnold Network\n",
        "Implement a classical Kolmogorov-Arnold Network using basis-splines or some other KAN architecture and apply it to MNIST. Show its performance on the test data. Comment on potential ideas to extend this classical KAN architecture to a quantum KAN and sketch out the architecture in detail.\n"
      ],
      "metadata": {
        "id": "c4tTG9Wlj__c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fourier Kolmogorov-Arnold Networks (KANs)\n",
        "Kolmogorov-Arnold Networks are neural networks inspired by the Kolmogorov-Arnold representation theorem, which states that any multivariate continuous function can be represented using a finite sum of one-dimensional functions. The Fourier KAN leverages this concept using Fourier series expansions to approximate complex functions."
      ],
      "metadata": {
        "id": "SVa8BiwNkO2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Necessary Libraries"
      ],
      "metadata": {
        "id": "H7Ey53LakmI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wmkOKDyLe9UE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Fourier KAN Layer\n",
        "The NaiveFourierKANLayer is a custom layer that applies a Fourier series expansion to its input. Instead of traditional linear transformations, it uses sine and cosine functions to model non-linear patterns."
      ],
      "metadata": {
        "id": "VoARu5mpkt_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveFourierKANLayer(nn.Module):\n",
        "    def __init__(self, inputdim, outdim, initial_gridsize, addbias=True):\n",
        "        super(NaiveFourierKANLayer, self).__init__()\n",
        "        self.addbias = addbias\n",
        "        self.inputdim = inputdim\n",
        "        self.outdim = outdim\n",
        "\n",
        "        # Learnable gridsize parameter\n",
        "        self.gridsize_param = nn.Parameter(torch.tensor(initial_gridsize, dtype=torch.float32))\n",
        "\n",
        "        # Fourier coefficients as a learnable parameter with Xavier initialization\n",
        "        self.fouriercoeffs = nn.Parameter(torch.empty(2, outdim, inputdim, initial_gridsize))\n",
        "        nn.init.xavier_uniform_(self.fouriercoeffs)\n",
        "\n",
        "        if self.addbias:\n",
        "            self.bias = nn.Parameter(torch.zeros(1, outdim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        gridsize = torch.clamp(self.gridsize_param, min=1).round().int()\n",
        "        xshp = x.shape\n",
        "        outshape = xshp[:-1] + (self.outdim,)\n",
        "        x = torch.reshape(x, (-1, self.inputdim))\n",
        "        k = torch.reshape(torch.arange(1, gridsize + 1, device=x.device), (1, 1, 1, gridsize))\n",
        "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
        "        c = torch.cos(k * xrshp)\n",
        "        s = torch.sin(k * xrshp)\n",
        "        y = torch.sum(c * self.fouriercoeffs[0:1, :, :, :gridsize], (-2, -1))\n",
        "        y += torch.sum(s * self.fouriercoeffs[1:2, :, :, :gridsize], (-2, -1))\n",
        "        if self.addbias:\n",
        "            y += self.bias\n",
        "        y = torch.reshape(y, outshape)\n",
        "        return y"
      ],
      "metadata": {
        "id": "N74tN_hke9Ql"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTFourierKAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTFourierKAN, self).__init__()\n",
        "        self.fourierkan1 = NaiveFourierKANLayer(28*28, 128, initial_gridsize=28)\n",
        "        self.fourierkan2 = NaiveFourierKANLayer(128, 10, initial_gridsize=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the images\n",
        "        x = self.fourierkan1(x)\n",
        "        x = self.fourierkan2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gqL2lcSpe9N0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the MNIST dataset"
      ],
      "metadata": {
        "id": "4Z4-kNUQfjMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "EaNVbW8_e8_q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# smaller subset for the training dataset to speed up training"
      ],
      "metadata": {
        "id": "arXCOKY8fqkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_indices = np.random.choice(len(train_dataset), int(len(train_dataset) * 0.1), replace=False)\n",
        "train_subset = Subset(train_dataset, subset_indices)\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "CLdWCdp4fpgQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the model and optimizer with a lower learning rate"
      ],
      "metadata": {
        "id": "GZXUFx1bgb0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MNISTFourierKAN().to(device)\n",
        "optimizer = optim.LBFGS(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "X0clClvuf-Gb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "2Gx34OcSghFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0lW8HHpTeaGa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48cfbad4-3e5c-4755-fb5d-f1c700fee28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.046352\n",
            "Train Epoch: 1 [640/6000 (11%)]\tLoss: 0.715492\n",
            "Train Epoch: 1 [1280/6000 (21%)]\tLoss: 0.449179\n",
            "Train Epoch: 1 [1920/6000 (32%)]\tLoss: 0.411832\n",
            "Train Epoch: 1 [2560/6000 (43%)]\tLoss: 0.261331\n",
            "Train Epoch: 1 [3200/6000 (53%)]\tLoss: 0.356095\n",
            "Train Epoch: 1 [3840/6000 (64%)]\tLoss: 0.246081\n",
            "Train Epoch: 1 [4480/6000 (74%)]\tLoss: 0.245384\n",
            "Train Epoch: 1 [5120/6000 (85%)]\tLoss: 0.328825\n",
            "Train Epoch: 1 [5760/6000 (96%)]\tLoss: 0.284281\n",
            "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.132047\n",
            "Train Epoch: 2 [640/6000 (11%)]\tLoss: 0.150725\n",
            "Train Epoch: 2 [1280/6000 (21%)]\tLoss: 0.116708\n",
            "Train Epoch: 2 [1920/6000 (32%)]\tLoss: 0.112002\n",
            "Train Epoch: 2 [2560/6000 (43%)]\tLoss: 0.192524\n",
            "Train Epoch: 2 [3200/6000 (53%)]\tLoss: 0.086362\n",
            "Train Epoch: 2 [3840/6000 (64%)]\tLoss: 0.068165\n",
            "Train Epoch: 2 [4480/6000 (74%)]\tLoss: 0.186346\n",
            "Train Epoch: 2 [5120/6000 (85%)]\tLoss: 0.120741\n",
            "Train Epoch: 2 [5760/6000 (96%)]\tLoss: 0.080852\n"
          ]
        }
      ],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = nn.CrossEntropyLoss()(output, target)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.step(closure)\n",
        "        if batch_idx % 10 == 0:\n",
        "            loss = closure()\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device , train_loader, optimizer, epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance on the test set\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calculate and print the average loss and accuracy\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYxHy5h7C-Mb",
        "outputId": "b7b3887d-2249-4eab-9feb-c668ab438daf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0013, Accuracy: 9172/10000 (92%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}